{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL Analysis\n",
    "##### David M. Freestone & Fuat Balci\n",
    "March 14, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook shows how to analyze the data collected using the Psychopy DRL experiment accompanying xxx (the chapter).\n",
    "\n",
    "First, we need to tell Python what packages and functions we'll use in this analysis script. They come in the form of import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob     # for finding data files\n",
    "from os.path import join  # for dealing with path names easily\n",
    "\n",
    "from pandas import read_csv, concat, isnull     # reading in the data\n",
    "from scipy.stats import invgauss, expon # useful distributions\n",
    "from scipy.optimize import minimize     # gradient descent fitting\n",
    "from numpy import log, sqrt, hstack     # some math functions\n",
    "\n",
    "from matplotlib.pyplot import subplots, style  # for plotting\n",
    "style.use(\"fivethirtyeight\")                   # set figure style\n",
    "\n",
    "# Import entire packages, so we can explore later.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Needed so plots show up in the jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Matplotlib spits out an annoying FutureWarning that we don't care about.\n",
    "# Suppress it\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the data directory, find all the \".csv\" files in the directory, and load them all into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e742e3315219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/dmf025/anaconda/lib/python3.4/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    832\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                        copy=copy)\n\u001b[0m\u001b[1;32m    835\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmf025/anaconda/lib/python3.4/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No objects to concatenate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "data_directory = \"data\"\n",
    "data_files = glob(join(data_directory, \"*.csv\"))\n",
    "data = concat([read_csv(f) for f in data_files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of columns in this dataset that we'll probably never use (i.e., frameRate). It makes looking at the data easier if we remove them.\n",
    "\n",
    "We'll also remove any event that isn't a response (like the introduction and break screens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = [\"participant\", \"session\", \"date\", \"time\", \n",
    "           \"block_number\", \"interval\", \"response_number\", \"rt\", \"score\"]\n",
    "response_idx = data.response_number > 0\n",
    "data = data.ix[response_idx, columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Summary measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group = [\"participant\", \"session\"]\n",
    "data.groupby(group).rt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.groupby(group).rt.agg([\"mean\", \"std\", \"sem\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = data.participant==\"DMF\"\n",
    "x = data[idx].response_number\n",
    "y = data[idx].rt\n",
    "\n",
    "fig, ax = subplots()\n",
    "ax.scatter(x, y, s=50)\n",
    "ax.hlines(data[idx].interval, 0, 1+x.max(),\n",
    "          linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "# Modify the axes\n",
    "ax.set_xlim((0, 1+x.max()))\n",
    "ax.set_xlabel(\"Response number\", fontsize=22)\n",
    "ax.set_ylabel(\"Interresponse time (s)\", fontsize=22)\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot a histogram of the response times\n",
    "ax = data[data.participant==\"DMF\"].rt.hist();\n",
    "\n",
    "# plot a line at the DRL schedule\n",
    "ymin, ymax = ax.get_ylim()\n",
    "ax.vlines(5, ymin, ymax, \n",
    "          linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "# Label the x and y axes, and change the tick label size\n",
    "ax.set_xlabel(\"Interresponse time\", fontsize=22)\n",
    "ax.set_ylabel(\"Count\", fontsize=22)\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Fitting the interresponse time distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In humans, the interresponse time is often close to normally distributed, although an inverse gaussian distribution is slightly better. In rats, there are often many \"untimed\" responses that follow an exponential distribution.\n",
    "\n",
    "While we probably don't need to fit a mixture distribution here, we do so for the purposes of this tutorial.\n",
    "\n",
    "The interresponse time distribution is a mixture of \"timed\" and \"untimed\" interresponse times, with some fraction ($p$) coming from the timed distribution (inverse gaussian) and the other fraction ($1-p$) coming from an exponential.\n",
    "\n",
    "$$p(x) = pIG(x; \\mu, \\lambda_{IG}) + (1-p)\\lambda e^{-\\lambda_{e}t}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$IG(x; \\mu, \\lambda_{IG}) = \\sqrt{\\frac{\\lambda_{IG}}{2\\pi x^3}}e^{\\frac{\\lambda_{IG}(x-\\mu)^2}{2\\mu^2x}}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\lambda_{IG} = \\frac{\\mu}{\\gamma^2}$$\n",
    "\n",
    "where $\\gamma$ is the weber fraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Estimate the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import invgauss, expon # import the two functions we need\n",
    "IGpdf = invgauss.pdf\n",
    "exppdf = expon.pdf\n",
    "\n",
    "def expIG(x, p, μ, λ_IG, λ_e):\n",
    "    \"\"\"Return the expIG evaluated at x\n",
    "        Note that this is in \"standardized form\n",
    "        so that mu = μ\\λ_IG.\n",
    "    \"\"\"\n",
    "    return p*IGpdf(x, mu=μ/λ_IG, scale=λ_IG) + (1-p)*exppdf(x, λ_e)\n",
    "\n",
    "def weber(μ, λ):\n",
    "    return sqrt(μ/λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use gradient descent algorithms to get the maximum likely parameters of the expIG mixture distribution. We'll use the \"Nelder-Mead\" simplex method because it does not require estimating the likelihood function. The drawback to this method is that constraining the parameters is not a straightforward issue. We'll use the simplest method, because its good enough for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_likelihood(params, x):\n",
    "    \"\"\"Return the log likelihood of x\"\"\"\n",
    "    p, μ, λ_IG, λ_e = params\n",
    "    \n",
    "    return log(expIG(x, p, μ, λ_IG, λ_e)).sum()\n",
    "\n",
    "def negative_llk(params, x):\n",
    "    \"\"\"Return the negative log-likelihood of x\n",
    "        or 100000 (a really big number) if the \n",
    "        constraints are not met\n",
    "    \"\"\"\n",
    "    if constraints(params):\n",
    "        return -log_likelihood(params, x)\n",
    "    else:\n",
    "        return 100000\n",
    "    \n",
    "def constraints(params):\n",
    "    \"\"\"Return True if the constraints are met, else False\"\"\"\n",
    "    p, μ, λ_IG, λ_e = params\n",
    "    if (p < 0) or (p > 1): return False\n",
    "    if μ < 0: return False\n",
    "    if λ_IG < 0: return False\n",
    "    if λ_e < 0: return False\n",
    "    return True\n",
    "\n",
    "def fit(D, p0):\n",
    "    \"\"\"Return the fitted params, \n",
    "        plus the -LLK and whether the fit succeeded\n",
    "    \"\"\"\n",
    "    result = minimize(negative_llk, p0, args=D, method=\"Nelder-Mead\")\n",
    "    return hstack((result.x, result.fun, result.success))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit each participant, pooling over sessions, and add the weber fraction ($\\gamma$) to the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_estimate = [0.9, 5, 100, 100]\n",
    "\n",
    "# Do the fitting on each participant.\n",
    "fit_results = data.groupby(\"participant\").rt.apply(fit, initial_estimate)\n",
    "\n",
    "# Prettify the DataFrame\n",
    "fit_results = fit_results.apply(pd.Series)\n",
    "fit_results.columns = [\"p\", \"μ\", \"λ_IG\", \"λ_e\", \"negLLK\", \"Success\"]\n",
    "\n",
    "# Add the weber fraction to the DataFrame\n",
    "fit_results[\"γ\"] = weber(fit_results[\"μ\"], fit_results[\"λ_IG\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Visualizing the fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "participant = \"DMF\"\n",
    "# Pull out the parameters for one participant (and convert to a matrix)\n",
    "params = fit_results.ix[participant, [\"p\", \"μ\", \"λ_IG\", \"λ_e\"]].as_matrix()\n",
    "\n",
    "x = np.linspace(0, 15, 1000)\n",
    "\n",
    "rt = data[data.participant==participant].rt.as_matrix()\n",
    "rt_fit = expIG(x, *params)\n",
    "\n",
    "# Plot\n",
    "fig, ax = subplots()\n",
    "ax.hist(rt, bins=10, normed=True);\n",
    "ax.plot(x, rt_fit)\n",
    "\n",
    "# Modify the axes\n",
    "ax.set_xlabel(\"Interresponse time (s)\", fontsize=22)\n",
    "ax.set_ylabel(\"Probability\", fontsize=22)\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Assessing Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume optimal behavior is to maximize the reward rate. The reward rate on this task is the probability of obtaining a reward divided by the time between rewards:\n",
    "\n",
    "$$\\frac{p(R)}{\\mu}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$p(R) = p(t>T)$$\n",
    "\n",
    "that is, the probability of a reward is just the fraction of trials in which the interresponse time $t$ was greater than the DRL schedule $T$. Assuming the timed portion of the interresponse time distribution follows an inverse gaussian, the probability of a reward is\n",
    "\n",
    "$$= 1-p(t\\le T)$$\n",
    "\n",
    "$$= 1-IG_{cdf}(T; \\mu, \\lambda_{IG})$$\n",
    "\n",
    "So that the expected reward rate is:\n",
    "$$E[r] = \\frac{1-IG_{cdf}(T; \\mu, \\lambda_{IG})}{\\mu}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IGcdf = invgauss.cdf\n",
    "def p_reward(T, μ, γ):\n",
    "    λ_IG = μ/(γ**2)\n",
    "    return 1 - IGcdf(T, mu=μ/λ_IG, scale=λ_IG)\n",
    "\n",
    "def reward_rate(T, μ, γ):\n",
    "    return p_reward(T, μ, γ) / μ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal reward rate is the mean interresponse time that maximizes the reward rate\n",
    "\n",
    "$$\\mathrm{argmax}_\\mu E[r; \\gamma]$$\n",
    "\n",
    "The notation shows the dependence of $\\gamma$. This equation gives the reward maximizing relationship between $\\mu$ and $\\gamma$. Because its a function of $\\gamma$, the reward maximizing strategy is a infinite set of points that lay along an \"optimal performance curve\" ($OPC$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimal(T, γ):\n",
    "    \"\"\"Return the optimal mean interresponse time given γ\"\"\"\n",
    "    μ, γ = np.meshgrid(np.linspace(0, 3*T, 1000), γ)\n",
    "    r = reward_rate(T, μ, γ)\n",
    "\n",
    "    # The first few γ values have to be thrown out because there's no good way to get the\n",
    "    # reward rate at very low γ.\n",
    "    idx = ~np.isnan(r).all(axis=1)\n",
    "    γ = γ[idx, :]\n",
    "    r = r[idx, :]\n",
    "    return (μ[0, np.nanargmax(r, axis=1)], \n",
    "            np.nanmax(r, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward rate grows over $\\mu$ because the probability of a reward grows as the mean interresponse time grows. But the reward rate shrinks over $\\mu$ because the time between rewards grows as the mean interresponse time grows. The $\\mu$ at which their influences cross is the reward maximizing interresponse time. This depends on the participant's timing variability $\\gamma$ because $\\gamma$ governs the rate at which the probability of a reward grows over $\\mu$.\n",
    "\n",
    "Here is a plot of the reward rate for different values of $\\gamma$. We also plot the Optimal Performance Curve (in black) that shows the reward rate earned for an optimal participant, given their timing variability $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T = 5\n",
    "μ = np.linspace(0, 15, 1000)\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "fig, ax = subplots()\n",
    "\n",
    "ax.vlines(T, 0, 0.2, linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "ax.plot(μ, reward_rate(T, μ, 0.2), label=r\"$\\gamma = 0.2$\")\n",
    "ax.plot(μ, reward_rate(T, μ, 0.3), label=r\"$\\gamma = 0.3$\")\n",
    "ax.plot(μ, reward_rate(T, μ, 0.4), label=r\"$\\gamma = 0.4$\")\n",
    "\n",
    "μ_opt, r_opt = optimal(5, np.linspace(0, 1.2, 100))\n",
    "ax.plot(μ_opt, r_opt, color='k', label=\"$OPC$\")\n",
    "\n",
    "ax.legend(loc=\"best\", fontsize=18, framealpha=0)\n",
    "ax.set_xlabel(\"Mean interresponse time, $\\mu$\", fontsize=22)\n",
    "ax.set_ylabel(\"Reward rate\", fontsize=22)\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Are the participants optimal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the degree to which the participants are optimal, we have to find the optimal mean interresponse time per participant, and then compare it to their actual mean interresponse time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "γ = fit_results.γ.as_matrix()\n",
    "μ = fit_results.μ.as_matrix()\n",
    "r = reward_rate(5, μ, γ)\n",
    "\n",
    "μ_opt, r_opt = optimal(5, γ)\n",
    "\n",
    "distance_from_optimal = μ / μ_opt\n",
    "proportion_maximum_gain = r / r_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot these two measures over participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "participants = fit_results.index.values\n",
    "nparticipants = len(participants)\n",
    "\n",
    "fig, ax = subplots(ncols=2, figsize=(9,5))\n",
    "ax_dist, ax_prop = ax\n",
    "\n",
    "# Plot the distance from optimal measure.\n",
    "ax_dist.bar(range(0,nparticipants), distance_from_optimal, align=\"center\");\n",
    "ax_dist.set_ylabel(\"distance from optimal\", fontsize=22)\n",
    "\n",
    "# Plot the distance from optimal measure.\n",
    "ax_prop.bar(range(0,nparticipants), proportion_maximum_gain, align=\"center\");\n",
    "ax_prop.set_ylabel(\"proportion maximum gain\", fontsize=22)\n",
    "\n",
    "# Mofidy both axes at the same time.\n",
    "for a in ax:\n",
    "    a.set_xticks(range(0, nparticipants))\n",
    "    a.set_xticklabels(participants)\n",
    "    a.set_ylim((0.9, 1))\n",
    "    a.set_xlabel(\"Participant\", fontsize=22)\n",
    "    a.tick_params(labelsize=20)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
